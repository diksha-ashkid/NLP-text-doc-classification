{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Dev Chakravarty\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\Dev Chakravarty\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n",
      "c:\\Users\\Dev Chakravarty\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\backend.py:5575: UserWarning: \"`categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n",
      "  output, from_logits = _get_logits(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Dev Chakravarty\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "38/38 [==============================] - 2033s 52s/step - loss: 0.6973 - balanced_accuracy: 0.7644 - val_loss: 0.0857 - val_balanced_accuracy: 0.9800\n",
      "4/4 [==============================] - 45s 9s/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.95      0.95        19\n",
      "           1       1.00      1.00      1.00         3\n",
      "           2       1.00      1.00      1.00         3\n",
      "           3       1.00      1.00      1.00        20\n",
      "           4       1.00      1.00      1.00        28\n",
      "           5       0.96      0.96      0.96        27\n",
      "\n",
      "    accuracy                           0.98       100\n",
      "   macro avg       0.99      0.99      0.99       100\n",
      "weighted avg       0.98      0.98      0.98       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "#only take 1367 rows randomly from the dataset\n",
    "df = pd.read_csv(r'G:\\Diksha\\7 sem\\finalyrpro\\NLP-text-doc-classification\\Dataset\\combined_final_dataset.csv')\n",
    "df = df.sample(n=1367, random_state=1)\n",
    "df.to_csv('combined_final_dataset_train.csv', index=False)\n",
    "\n",
    "\n",
    "#only take 100 rows randomly from the dataset\n",
    "df = pd.read_csv(r'G:\\Diksha\\7 sem\\finalyrpro\\NLP-text-doc-classification\\Dataset\\combined_final_dataset.csv')\n",
    "df = df.sample(n=100, random_state=1)\n",
    "df.to_csv('combined_final_dataset_test.csv', index=False)\n",
    "\n",
    "\n",
    "df_train = pd.read_csv('combined_final_dataset_train.csv')\n",
    "df_test = pd.read_csv('combined_final_dataset_test.csv')\n",
    "\n",
    "\n",
    "#drop columns Title. category and date\n",
    "df_test = df_test.drop(['Title','category','date'],axis = 1)\n",
    "df_train = df_train.drop(['Title','category','date'],axis = 1)\n",
    "\n",
    "# Assuming df_train and df_test are your DataFrames\n",
    "encoded_dict = {\n",
    "    'News': 0,\n",
    "    'Research Paper': 1,\n",
    "    'Code': 2,\n",
    "    'Medical': 3,\n",
    "    'Legal': 4,\n",
    "    'Financial documents': 5\n",
    "}\n",
    "\n",
    "df_train['main_category'] = df_train['main_category'].map(encoded_dict)\n",
    "df_test['main_category'] = df_test['main_category'].map(encoded_dict)\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "y_train = to_categorical(df_train.main_category)\n",
    "y_test = to_categorical(df_test.main_category)\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoTokenizer,TFBertModel\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n",
    "bert = TFBertModel.from_pretrained('bert-base-cased')\n",
    "\n",
    "#datatypes of description\n",
    "df_train.description.dtype\n",
    "#convert desription column to string\n",
    "df_train['description'] = df_train['description'].astype(str)\n",
    "df_test['description'] = df_test['description'].astype(str)\n",
    "\n",
    "# Tokenize the input (takes some time) \n",
    "# here tokenizer using from bert-base-cased\n",
    "x_train = tokenizer(\n",
    "    text=df_train.description.tolist(),\n",
    "    add_special_tokens=True,\n",
    "    max_length=300,\n",
    "    truncation=True,\n",
    "    padding=True, \n",
    "    return_tensors='tf',\n",
    "    return_token_type_ids = False,\n",
    "    return_attention_mask = True,\n",
    "    verbose = True)\n",
    "x_test = tokenizer(\n",
    "    text=df_test.description.tolist(),\n",
    "    add_special_tokens=True,\n",
    "    max_length=300,\n",
    "    truncation=True,\n",
    "    padding=True, \n",
    "    return_tensors='tf',\n",
    "    return_token_type_ids = False,\n",
    "    return_attention_mask = True,\n",
    "    verbose = True)\n",
    "\n",
    "input_ids = x_train['input_ids']\n",
    "attention_mask = x_train['attention_mask']\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.initializers import TruncatedNormal\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.metrics import CategoricalAccuracy\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "\n",
    "max_len = 300\n",
    "input_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_ids\")\n",
    "input_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"attention_mask\")\n",
    "embeddings = bert(input_ids,attention_mask = input_mask)[0] \n",
    "out = tf.keras.layers.GlobalMaxPool1D()(embeddings)\n",
    "out = Dense(128, activation='relu')(out)\n",
    "out = tf.keras.layers.Dropout(0.1)(out)\n",
    "out = Dense(32,activation = 'relu')(out)\n",
    "y = Dense(6,activation = 'sigmoid')(out)\n",
    "model = tf.keras.Model(inputs=[input_ids, input_mask], outputs=y)\n",
    "model.layers[2].trainable = True\n",
    "\n",
    "optimizer = Adam(\n",
    "    learning_rate=5e-05, # this learning rate is for bert model , taken from huggingface website \n",
    "    epsilon=1e-08,\n",
    "    clipnorm=1.0)\n",
    "# Set loss and metrics\n",
    "loss =CategoricalCrossentropy(from_logits = True)\n",
    "metric = CategoricalAccuracy('balanced_accuracy'),\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer = optimizer,\n",
    "    loss = loss, \n",
    "    metrics = metric)\n",
    "\n",
    "train_history = model.fit(\n",
    "    x ={'input_ids':x_train['input_ids'],'attention_mask':x_train['attention_mask']} ,\n",
    "    y = y_train,\n",
    "    validation_data = (\n",
    "    {'input_ids':x_test['input_ids'],'attention_mask':x_test['attention_mask']}, y_test\n",
    "    ),\n",
    "  epochs=1,\n",
    "    batch_size=36\n",
    ")\n",
    "\n",
    "predicted_raw = model.predict({'input_ids':x_test['input_ids'],'attention_mask':x_test['attention_mask']})\n",
    "predicted_raw[0]\n",
    "\n",
    "import numpy as np\n",
    "y_predicted = np.argmax(predicted_raw, axis = 1)\n",
    "y_true = df_test.main_category\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_true, y_predicted))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"#validation = model.predict({'input_ids':x_val['input_ids'],'attention_mask':x_val['attention_mask']})*100\\nfor key , value in zip(encoded_dict.keys(),validation[0]):\\n    print(key,value)\\n    #ssvae the value ina  variable\\n    dict1 = {key:value}\\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import logging\n",
    "from dir_ops import Move\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from SearchAlgo import DocumentSearch\n",
    "from transformers import AutoTokenizer,TFBertModel #importing the tokenizer and bert model\n",
    "#import keras\n",
    "import tensorflow as tf\n",
    "from keras.models import load_model\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n",
    "bert = TFBertModel.from_pretrained('bert-base-cased')\n",
    "\n",
    "\n",
    "#model = load_model(\"D:/Backup/Desktop/programs/NLP-text-doc-classification/customized_transformer_model.h5\",custom_objects={\"TFBertModel\":TFBertModel})\n",
    "\n",
    "'''#validation = model.predict({'input_ids':x_val['input_ids'],'attention_mask':x_val['attention_mask']})*100\n",
    "for key , value in zip(encoded_dict.keys(),validation[0]):\n",
    "    print(key,value)\n",
    "    #ssvae the value ina  variable\n",
    "    dict1 = {key:value}\n",
    "'''\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On the solemn anniversary of 26/11, the world reflects on the tragic events that unfolded in Mumbai, India, in 2008. This day marks the 13th anniversary of the coordinated terrorist attacks that shook the nation, claiming the lives of 166 people and injuring hundreds more. The attacks targeted multiple locations, including the iconic Taj Mahal Palace Hotel, the Oberoi Trident Hotel, and the Chhatrapati Shivaji Terminus. Commemorative events, memorial services, and tributes are held to honor the victims and salute the resilience of the city. The legacy of 26/11 serves as a stark reminder of the persistent global challenges posed by terrorism and underscores the importance of international cooperation in combating extremism and ensuring the safety and security of communities worldwide\n",
      "1/1 [==============================] - 1s 724ms/step\n",
      "News 87.49363\n",
      "Research Paper 31.51227\n",
      "Code 32.221413\n",
      "Medical 22.177235\n",
      "Legal 44.891598\n",
      "Financial documents 64.0305\n"
     ]
    }
   ],
   "source": [
    "ex_file = r\"G:\\Diksha\\7 sem\\finalyrpro\\NLP-text-doc-classification\\demo\\news doc.txt\"\n",
    "\n",
    "#extract text from file and store in a variable\n",
    "with open(ex_file, 'r') as f:\n",
    "    text = f.read()\n",
    "print(text)\n",
    "encoded_dict = {\n",
    "    'News': 0,\n",
    "    'Research Paper': 1,\n",
    "    'Code': 2,\n",
    "    'Medical': 3,\n",
    "    'Legal': 4,\n",
    "    'Financial documents': 5\n",
    "}\n",
    "\n",
    "x_val = tokenizer(\n",
    "    text=text,\n",
    "    add_special_tokens=True,\n",
    "    max_length=300,\n",
    "    truncation=True,\n",
    "    padding='max_length', \n",
    "    return_tensors='tf',\n",
    "    return_token_type_ids = False,\n",
    "    return_attention_mask = True,\n",
    "    verbose = True)\n",
    "\n",
    "validation = model.predict({'input_ids':x_val['input_ids'],'attention_mask':x_val['attention_mask']})*100\n",
    "#validation = model.predict({'input_ids':x_val['input_ids'],'attention_mask':x_val['attention_mask']})*100\n",
    "for key , value in zip(encoded_dict.keys(),validation[0]):\n",
    "   print(key,value)\n",
    "#save the value ina  variable\n",
    "   dict1 = {key:value}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Financial documents\n"
     ]
    }
   ],
   "source": [
    "#get maximum value in the dict\n",
    "max_key = max(dict1, key=dict1.get)\n",
    "print(max_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Financial Documents\n"
     ]
    }
   ],
   "source": [
    "#select the key with the highest value\n",
    "max_key = max(dict1, key=d).title()\n",
    "print(max_key)\n",
    "#max_key = \"Research Paper\"\n",
    "mover = Move()\n",
    "mover.classify_document(ex_file, max_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "searcher = DocumentSearch()\n",
    "\n",
    "user_keyword = input(\"comma separated keywords: \").split(\",\")\n",
    "results = searcher.retrieve_documents(user_keyword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Retrieved Documents:\")\n",
    "for i, doc in enumerate(results, start=1,):\n",
    "    print(f\"{i}. Title: {doc['Title']}, Category: {doc['main_category']}\")# Date: {doc['date']}, Description: x\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
