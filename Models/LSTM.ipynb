{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.layers import Embedding, Bidirectional, LSTM, Dropout, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Code': 0, 'Medical': 1, 'News': 2, 'Research Paper': 3}\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('../Dataset/combined_final_dataset.csv')\n",
    "\n",
    "le = LabelEncoder()\n",
    "le.fit(data[\"main_category\"])\n",
    "le_mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "z = data.pop(\"date\")\n",
    "del(z)\n",
    "m = data.pop(\"category\")\n",
    "del(m)\n",
    "n = data.pop(\"Title\")\n",
    "del(n)\n",
    "data[\"main_category\"] = LabelEncoder().fit_transform(data[\"main_category\"])\n",
    "y = data.pop('main_category')\n",
    "print(le_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         description\n",
      "0  health expert said early predict whether deman...\n",
      "1  subdued passenger crew fled back aircraft conf...\n",
      "2                         dog understand could eaten\n",
      "3  accidentally put toothpaste toothbrush screame...\n",
      "4  amy cooper accused investment firm franklin te...\n",
      "Index(['description'], dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1    349\n",
       "2    324\n",
       "3     61\n",
       "0     33\n",
       "Name: main_category, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(data.head())\n",
    "print(data.columns)\n",
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(613, 1)\n",
      "(154, 1)\n",
      "(613,)\n",
      "(154,)\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "x_train, x_test, y_train, y_test = train_test_split(data, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "#convert to numpy array\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 20000\n",
    "embedding_dim = 32\n",
    "max_length = 200\n",
    "trunc_type='post'\n",
    "padding_type='pre'\n",
    "\n",
    "tokenizer = Tokenizer(num_words = vocab_size, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape\n",
    "x_train_sentences = x_train['description'].tolist()\n",
    "x_test_sentences = x_test['description'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = tokenizer.texts_to_sequences(x_train_sentences)\n",
    "padded = pad_sequences(sequences,maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    "test_sequences = tokenizer.texts_to_sequences(x_test_sentences)\n",
    "test_padded = pad_sequences(test_sequences,maxlen=max_length, padding=padding_type, truncating=trunc_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(613, 200)\n"
     ]
    }
   ],
   "source": [
    "print(padded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(4, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_4 (Embedding)     (None, 200, 32)           640000    \n",
      "                                                                 \n",
      " flatten_4 (Flatten)         (None, 6400)              0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 128)               819328    \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 4)                 516       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1459844 (5.57 MB)\n",
      "Trainable params: 1459844 (5.57 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "#\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 1s 33ms/step - loss: 0.7086 - accuracy: 0.8010 - val_loss: 0.4768 - val_accuracy: 0.8377\n",
      "Epoch 2/30\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 0.4200 - accuracy: 0.8744 - val_loss: 0.4008 - val_accuracy: 0.8442\n",
      "Epoch 3/30\n",
      "20/20 [==============================] - 0s 24ms/step - loss: 0.3781 - accuracy: 0.8825 - val_loss: 0.3823 - val_accuracy: 0.8636\n",
      "Epoch 4/30\n",
      "20/20 [==============================] - 1s 26ms/step - loss: 0.3538 - accuracy: 0.8825 - val_loss: 0.3680 - val_accuracy: 0.8506\n",
      "Epoch 5/30\n",
      "20/20 [==============================] - 0s 24ms/step - loss: 0.3570 - accuracy: 0.8809 - val_loss: 0.3674 - val_accuracy: 0.8636\n",
      "Epoch 6/30\n",
      "20/20 [==============================] - 1s 25ms/step - loss: 0.3464 - accuracy: 0.8874 - val_loss: 0.3866 - val_accuracy: 0.8636\n",
      "Epoch 7/30\n",
      "20/20 [==============================] - 0s 24ms/step - loss: 0.3554 - accuracy: 0.8825 - val_loss: 0.3838 - val_accuracy: 0.8506\n",
      "Epoch 8/30\n",
      "20/20 [==============================] - 1s 27ms/step - loss: 0.3492 - accuracy: 0.8842 - val_loss: 0.3679 - val_accuracy: 0.8636\n",
      "Epoch 9/30\n",
      "20/20 [==============================] - 0s 25ms/step - loss: 0.3541 - accuracy: 0.8858 - val_loss: 0.3724 - val_accuracy: 0.8636\n",
      "Epoch 10/30\n",
      "20/20 [==============================] - 0s 24ms/step - loss: 0.3423 - accuracy: 0.8874 - val_loss: 0.3570 - val_accuracy: 0.8636\n",
      "Epoch 11/30\n",
      "20/20 [==============================] - 1s 26ms/step - loss: 0.3491 - accuracy: 0.8874 - val_loss: 0.3718 - val_accuracy: 0.8701\n",
      "Epoch 12/30\n",
      "20/20 [==============================] - 0s 24ms/step - loss: 0.3368 - accuracy: 0.8809 - val_loss: 0.3722 - val_accuracy: 0.8636\n",
      "Epoch 13/30\n",
      "20/20 [==============================] - 0s 24ms/step - loss: 0.3292 - accuracy: 0.8891 - val_loss: 0.3670 - val_accuracy: 0.8701\n",
      "Epoch 14/30\n",
      "20/20 [==============================] - 0s 24ms/step - loss: 0.3311 - accuracy: 0.8858 - val_loss: 0.3599 - val_accuracy: 0.8701\n",
      "Epoch 15/30\n",
      "20/20 [==============================] - 0s 25ms/step - loss: 0.3317 - accuracy: 0.8891 - val_loss: 0.3676 - val_accuracy: 0.8636\n",
      "Epoch 16/30\n",
      "20/20 [==============================] - 0s 24ms/step - loss: 0.3312 - accuracy: 0.8891 - val_loss: 0.3628 - val_accuracy: 0.8636\n",
      "Epoch 17/30\n",
      "20/20 [==============================] - 1s 26ms/step - loss: 0.3294 - accuracy: 0.8891 - val_loss: 0.3736 - val_accuracy: 0.8701\n",
      "Epoch 18/30\n",
      "20/20 [==============================] - 1s 28ms/step - loss: 0.3270 - accuracy: 0.8874 - val_loss: 0.3644 - val_accuracy: 0.8636\n",
      "Epoch 19/30\n",
      "20/20 [==============================] - 0s 24ms/step - loss: 0.3370 - accuracy: 0.8891 - val_loss: 0.3801 - val_accuracy: 0.8636\n",
      "Epoch 20/30\n",
      "20/20 [==============================] - 1s 26ms/step - loss: 0.3241 - accuracy: 0.8858 - val_loss: 0.3717 - val_accuracy: 0.8636\n",
      "Epoch 21/30\n",
      "20/20 [==============================] - 0s 25ms/step - loss: 0.3248 - accuracy: 0.8874 - val_loss: 0.3600 - val_accuracy: 0.8701\n",
      "Epoch 22/30\n",
      "20/20 [==============================] - 1s 27ms/step - loss: 0.3311 - accuracy: 0.8907 - val_loss: 0.3721 - val_accuracy: 0.8701\n",
      "Epoch 23/30\n",
      "20/20 [==============================] - 0s 23ms/step - loss: 0.3293 - accuracy: 0.8907 - val_loss: 0.3808 - val_accuracy: 0.8701\n",
      "Epoch 24/30\n",
      "20/20 [==============================] - 0s 24ms/step - loss: 0.3190 - accuracy: 0.8891 - val_loss: 0.3661 - val_accuracy: 0.8701\n",
      "Epoch 25/30\n",
      "20/20 [==============================] - 0s 24ms/step - loss: 0.3289 - accuracy: 0.8874 - val_loss: 0.3865 - val_accuracy: 0.8701\n",
      "Epoch 26/30\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 0.3188 - accuracy: 0.8809 - val_loss: 0.3783 - val_accuracy: 0.8636\n",
      "Epoch 27/30\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 0.3327 - accuracy: 0.8809 - val_loss: 0.3676 - val_accuracy: 0.8701\n",
      "Epoch 28/30\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 0.3094 - accuracy: 0.8874 - val_loss: 0.3738 - val_accuracy: 0.8701\n",
      "Epoch 29/30\n",
      "20/20 [==============================] - 0s 23ms/step - loss: 0.3148 - accuracy: 0.8858 - val_loss: 0.4024 - val_accuracy: 0.8701\n",
      "Epoch 30/30\n",
      "20/20 [==============================] - 1s 25ms/step - loss: 0.3177 - accuracy: 0.8891 - val_loss: 0.3656 - val_accuracy: 0.8701\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x26444b2b040>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(padded, y_train, epochs=30, validation_data=(test_padded, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 42ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.14174145 0.6765909  0.00516331 0.17650425]\n",
      "Medical\n"
     ]
    }
   ],
   "source": [
    "new_description = [\"The conflict has led to the death of over 700 Israelis and thousands of others have been injured. Meanwhile, the death toll in Gaza has also risen to 493, suggest reports. US President Joe Biden \"]\n",
    "seq = tokenizer.texts_to_sequences(new_description)\n",
    "padded = pad_sequences(seq, maxlen=max_length)\n",
    "pred = model.predict(padded)\n",
    "print(pred[0])\n",
    "#convert num back to label\n",
    "print(list(le_mapping.keys())[list(le_mapping.values()).index(np.argmax(pred))])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Weights: {0: 5.675925925925926, 1: 0.5434397163120568, 2: 0.5916988416988417, 3: 3.4055555555555554}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import class_weight\n",
    "import numpy as np\n",
    "\n",
    "# Assuming 'y_train' contains your integer-encoded class labels\n",
    "class_labels = np.unique(y_train)\n",
    "\n",
    "# Calculate class weights\n",
    "class_weights = class_weight.compute_class_weight(class_weight = 'balanced', classes = class_labels, y = y_train)\n",
    "\n",
    "# Create a dictionary mapping class indices to their respective weights\n",
    "class_weight_dict = dict(zip(class_labels, class_weights))\n",
    "\n",
    "print(\"Class Weights:\", class_weight_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32md:\\Backup\\Desktop\\programs\\NLP-text-doc-classification\\Models\\LSTM.ipynb Cell 17\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Backup/Desktop/programs/NLP-text-doc-classification/Models/LSTM.ipynb#X22sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m pipe\u001b[39m.\u001b[39;49mto(\u001b[39m\"\u001b[39;49m\u001b[39mcuda\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Backup/Desktop/programs/NLP-text-doc-classification/Models/LSTM.ipynb#X22sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# if using torch < 2.0\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Backup/Desktop/programs/NLP-text-doc-classification/Models/LSTM.ipynb#X22sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# pipe.enable_xformers_memory_efficient_attention()\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Backup/Desktop/programs/NLP-text-doc-classification/Models/LSTM.ipynb#X22sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m prompt \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mAn astronaut riding a green horse\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\diffusers\\pipelines\\pipeline_utils.py:733\u001b[0m, in \u001b[0;36mDiffusionPipeline.to\u001b[1;34m(self, torch_device, torch_dtype, silence_dtype_warnings)\u001b[0m\n\u001b[0;32m    729\u001b[0m     logger\u001b[39m.\u001b[39mwarning(\n\u001b[0;32m    730\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThe module \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mmodule\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m has been loaded in 8bit and moving it to \u001b[39m\u001b[39m{\u001b[39;00mtorch_dtype\u001b[39m}\u001b[39;00m\u001b[39m via `.to()` is not yet supported. Module is still on \u001b[39m\u001b[39m{\u001b[39;00mmodule\u001b[39m.\u001b[39mdevice\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    731\u001b[0m     )\n\u001b[0;32m    732\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 733\u001b[0m     module\u001b[39m.\u001b[39;49mto(torch_device, torch_dtype)\n\u001b[0;32m    735\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m    736\u001b[0m     module\u001b[39m.\u001b[39mdtype \u001b[39m==\u001b[39m torch\u001b[39m.\u001b[39mfloat16\n\u001b[0;32m    737\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mstr\u001b[39m(torch_device) \u001b[39min\u001b[39;00m [\u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m    738\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m silence_dtype_warnings\n\u001b[0;32m    739\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_offloaded\n\u001b[0;32m    740\u001b[0m ):\n\u001b[0;32m    741\u001b[0m     logger\u001b[39m.\u001b[39mwarning(\n\u001b[0;32m    742\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mPipelines loaded with `torch_dtype=torch.float16` cannot run with `cpu` device. It\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    743\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m is not recommended to move them to `cpu` as running them will fail. Please make\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    746\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m `torch_dtype=torch.float16` argument, or use another device for inference.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    747\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\transformers\\modeling_utils.py:1902\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1897\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1898\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`.to` is not supported for `4-bit` or `8-bit` models. Please use the model as it is, since the\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1899\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m model has already been set to the correct devices and casted to the correct `dtype`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1900\u001b[0m     )\n\u001b[0;32m   1901\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1902\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mto(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1145\u001b[0m, in \u001b[0;36mModule.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1141\u001b[0m         \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   1142\u001b[0m                     non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[0;32m   1143\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m, non_blocking)\n\u001b[1;32m-> 1145\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_apply(convert)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    795\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[0;32m    796\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[1;32m--> 797\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[0;32m    799\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    800\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    801\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    802\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    807\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    808\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    795\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[0;32m    796\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[1;32m--> 797\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[0;32m    799\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    800\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    801\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    802\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    807\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    808\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    795\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[0;32m    796\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[1;32m--> 797\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[0;32m    799\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    800\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    801\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    802\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    807\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    808\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\module.py:820\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    816\u001b[0m \u001b[39m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[0;32m    817\u001b[0m \u001b[39m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[0;32m    818\u001b[0m \u001b[39m# `with torch.no_grad():`\u001b[39;00m\n\u001b[0;32m    819\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m--> 820\u001b[0m     param_applied \u001b[39m=\u001b[39m fn(param)\n\u001b[0;32m    821\u001b[0m should_use_set_data \u001b[39m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[0;32m    822\u001b[0m \u001b[39mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1143\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m   1140\u001b[0m \u001b[39mif\u001b[39;00m convert_to_format \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m t\u001b[39m.\u001b[39mdim() \u001b[39min\u001b[39;00m (\u001b[39m4\u001b[39m, \u001b[39m5\u001b[39m):\n\u001b[0;32m   1141\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   1142\u001b[0m                 non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[1;32m-> 1143\u001b[0m \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39;49mto(device, dtype \u001b[39mif\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_floating_point() \u001b[39mor\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_complex() \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m, non_blocking)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\cuda\\__init__.py:239\u001b[0m, in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    235\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m    236\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    237\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mmultiprocessing, you must use the \u001b[39m\u001b[39m'\u001b[39m\u001b[39mspawn\u001b[39m\u001b[39m'\u001b[39m\u001b[39m start method\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    238\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(torch\u001b[39m.\u001b[39m_C, \u001b[39m'\u001b[39m\u001b[39m_cuda_getDeviceCount\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m--> 239\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mTorch not compiled with CUDA enabled\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    240\u001b[0m \u001b[39mif\u001b[39;00m _cudart \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    241\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\n\u001b[0;32m    242\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
