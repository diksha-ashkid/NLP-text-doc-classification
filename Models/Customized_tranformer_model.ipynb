{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LOADING DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#creating train dataset using random rows from the combined dataset\n",
    "df = pd.read_csv(r'G:\\Diksha\\7 sem\\Final yearproject\\NLP-text-doc-classification\\Dataset\\combined_final_dataset.csv')\n",
    "df = df.sample(n=1367, random_state=1)\n",
    "#save the csv to dataset folder\n",
    "df.to_csv('combined_final_dataset_train.csv', index=False) #saving the training dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating test dataset using random rows from the combined dataset\n",
    "df = pd.read_csv(r'G:\\Diksha\\7 sem\\Final yearproject\\NLP-text-doc-classification\\Dataset\\combined_final_dataset.csv')\n",
    "df = df.sample(n=100, random_state=1)\n",
    "#save in datset folder\n",
    "\n",
    "df.to_csv('combined_final_dataset_test.csv', index=False) #saving the test csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('combined_final_dataset_train.csv') # reading the train csv\n",
    "df_test = pd.read_csv('combined_final_dataset_test.csv') # reading the test csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CLEANING DATA FOR NECESSARY USE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_test.drop(['Title','category','date'],axis = 1)#dropping the columns which are not required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.drop(['Title','category','date'],axis = 1) #dropping the columns which are not required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoding the main category column\n",
    "encoded_dict = {\n",
    "    'News': 0,\n",
    "    'Research Paper': 1,\n",
    "    'Code': 2,\n",
    "    'Medical': 3,\n",
    "    'Legal': 4,\n",
    "    'Financial documents': 5\n",
    "}\n",
    "#mapping the encoded values to the main category column\n",
    "df_train['main_category'] = df_train['main_category'].map(encoded_dict)\n",
    "df_test['main_category'] = df_test['main_category'].map(encoded_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Dev Chakravarty\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.utils import to_categorical "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PREPROCESSING THE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#one hot encoding the main category column\n",
    "y_train = to_categorical(df_train.main_category)\n",
    "y_test = to_categorical(df_test.main_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import AutoTokenizer,TFBertModel #importing the tokenizer and bert model\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased') #loading the tokenizer\n",
    "bert = TFBertModel.from_pretrained('bert-base-cased') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#datatypes of description\n",
    "df_train.description.dtype\n",
    "#convert desription column to string\n",
    "df_train['description'] = df_train['description'].astype(str)\n",
    "df_test['description'] = df_test['description'].astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizing the text--description column\n",
    "x_train = tokenizer(\n",
    "    text=df_train.description.tolist(),\n",
    "    add_special_tokens=True,\n",
    "    max_length=300,\n",
    "    truncation=True,\n",
    "    padding=True,  # Add padding to the text so it can be used as an input \n",
    "    return_tensors='tf',\n",
    "    return_token_type_ids = False,\n",
    "    return_attention_mask = True, # Returns a tensor containing the attention mask for the 2nd sentence of the pair if it exists, 0 otherwise.\n",
    "    verbose = True)\n",
    "x_test = tokenizer(\n",
    "    text=df_test.description.tolist(),\n",
    "    add_special_tokens=True,\n",
    "    max_length=300,\n",
    "    truncation=True,\n",
    "    padding=True, \n",
    "    return_tensors='tf',\n",
    "    return_token_type_ids = False,\n",
    "    return_attention_mask = True,\n",
    "    verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = x_train['input_ids']  #token ids\n",
    "attention_mask = x_train['attention_mask'] #attention mask is used to ignore the padded tokens in the sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.initializers import TruncatedNormal\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.metrics import CategoricalAccuracy\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Input, Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MODEL FINE TUNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 300  #maximum length of the sequence\n",
    "input_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_ids\")\n",
    "input_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"attention_mask\")\n",
    "embeddings = bert(input_ids,attention_mask = input_mask)[0] \n",
    "out = tf.keras.layers.GlobalMaxPool1D()(embeddings)  #pooling layer to reduce the dimensionality of the extracted features\n",
    "out = Dense(128, activation='relu')(out)\n",
    "out = tf.keras.layers.Dropout(0.1)(out)\n",
    "out = Dense(32,activation = 'relu')(out)\n",
    "y = Dense(6,activation = 'sigmoid')(out)\n",
    "model = tf.keras.Model(inputs=[input_ids, input_mask], outputs=y)\n",
    "model.layers[2].trainable = True #setting the bert layer to trainable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(\n",
    "    learning_rate=5e-05, # this learning rate is for bert model , taken from huggingface website \n",
    "    epsilon=1e-08,\n",
    "    clipnorm=1.0)\n",
    "# Set loss and metrics\n",
    "loss =CategoricalCrossentropy(from_logits = True)\n",
    "metric = CategoricalAccuracy('balanced_accuracy'),\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer = optimizer,\n",
    "    loss = loss, \n",
    "    metrics = metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_history = model.fit(\n",
    "    x ={'input_ids':x_train['input_ids'],'attention_mask':x_train['attention_mask']} , # input ids and attention mask are the inputs to the bert model\n",
    "    y = y_train, \n",
    "    validation_data = (\n",
    "    {'input_ids':x_test['input_ids'],'attention_mask':x_test['attention_mask']}, y_test\n",
    "    ),\n",
    "  epochs=1,\n",
    "    batch_size=36\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_raw = model.predict({'input_ids':x_test['input_ids'],'attention_mask':x_test['attention_mask']})\n",
    "predicted_raw[0] # predicted probabilities for the first row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "y_predicted = np.argmax(predicted_raw, axis = 1) # predicted class for the first row\n",
    "y_true = df_test.main_category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MODEL METRICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_true, y_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PEDICTION OF CLASS CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = 'Arrhythmia or irregular heartbeat is a condition in which the heart  is unable to pump blood to the body efficiently. Symptoms of arrhythmia include: Fluttering in the chest Pounding heartbeat'\n",
    "x_val = tokenizer(\n",
    "    text=texts,\n",
    "    add_special_tokens=True,\n",
    "    max_length=70,\n",
    "    truncation=True,\n",
    "    padding='max_length', \n",
    "    return_tensors='tf',\n",
    "    return_token_type_ids = False,\n",
    "    return_attention_mask = True,\n",
    "    verbose = True) \n",
    "validation = model.predict({'input_ids':x_val['input_ids'],'attention_mask':x_val['attention_mask']})*100\n",
    "for key , value in zip(encoded_dict.keys(),validation[0]):\n",
    "    print(key,value)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
